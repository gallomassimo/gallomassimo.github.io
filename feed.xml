<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-07-04T16:16:35+00:00</updated><id>/feed.xml</id><title type="html">Massimo Gallo</title><subtitle>Massimo Gallo is a Principal Engineer at Huawei Technologies Co.,Ltd in Paris since 2019.  He obtained the Ph.D. in Networks and computer science from Telecom ParisTech, Paris, France  in 2012, performing his graduate research at Orange Labs, France Telecom, Paris, France. He  spent six years as a researcher at BellLabs, Nokia working on Information Centric Networking  and High-speed packet processing. His work has been published in several top tier international  conferences (e.g., IEEE ICNP, Usenix ATC, ACM CoNEXT) and journals (e.g., Transaction on  Networking) and led to several patents. His main research interests are on the performance  evaluation, simulation, design and experimentation on networked systems with particular focus on  Programmable networks, Traffic generation, and Network Monitoring.</subtitle><author><name> </name><email>first.last@huawei.com</email></author><entry><title type="html">DUMBO accepted at Conext 2024</title><link href="/2024/01/15/Conext.html" rel="alternate" type="text/html" title="DUMBO accepted at Conext 2024" /><published>2024-01-15T00:00:00+00:00</published><updated>2024-01-15T00:00:00+00:00</updated><id>/2024/01/15/Conext</id><content type="html" xml:base="/2024/01/15/Conext.html"><![CDATA[<p>Our paper titled <em>“Taming the Elephants: Affordable Flow Length Prediction inthe Data Plane”</em> will be presented at Conext 2024</p>

<p>Congrats to the team, especially Raphael, Andrea, and Gabriele.</p>

<p><em>Abstract:</em> Machine Learning (ML) shows promising potential for enhancing networking tasks. In particular, early flow size prediction would be beneficial for a wide range of use cases. However, implementing an ML-enabled system is a challenging task due to network devices limited resources. Previous works have demonstrated the feasibility of running simple ML models in the data plane, yet their integration in a practical end-to-end system is not trivial. Additional challenges in resources management and model maintenance need to be addressed to ensure the network task(s) performance improvement justifies the system overhead. In this work, we propose DUMBO, a versatile end-to-end system to generate and exploit flow size hints at line rate.Our system seamlessly integrates and maintains a simple ML model that offers early coarse-grain flow size prediction in the data plane. We evaluate the proposed system on flow scheduling, per-flow packet inter-arrival time distribution, and flow size estimation using real traffic traces, and perform experiments using an FPGA prototype running on an AMD(R)-Xilinx(R) Alveo U280 SmartNIC. Our results show that DUMBO outperforms traditional state-of-the-art approaches by equipping network devices data planes with a lightweight ML model.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our paper titled “Taming the Elephants: Affordable Flow Length Prediction inthe Data Plane” will be presented at Conext 2024 Congrats to the team, especially Raphael, Andrea, and Gabriele. Abstract: Machine Learning (ML) shows promising potential for enhancing networking tasks. In particular, early flow size prediction would be beneficial for a wide range of use cases. However, implementing an ML-enabled system is a challenging task due to network devices limited resources. Previous works have demonstrated the feasibility of running simple ML models in the data plane, yet their integration in a practical end-to-end system is not trivial. Additional challenges in resources management and model maintenance need to be addressed to ensure the network task(s) performance improvement justifies the system overhead. In this work, we propose DUMBO, a versatile end-to-end system to generate and exploit flow size hints at line rate.Our system seamlessly integrates and maintains a simple ML model that offers early coarse-grain flow size prediction in the data plane. We evaluate the proposed system on flow scheduling, per-flow packet inter-arrival time distribution, and flow size estimation using real traffic traces, and perform experiments using an FPGA prototype running on an AMD(R)-Xilinx(R) Alveo U280 SmartNIC. Our results show that DUMBO outperforms traditional state-of-the-art approaches by equipping network devices data planes with a lightweight ML model.]]></summary></entry><entry><title type="html">Data Augmentation for Traffic Classification accepted at PAM 2024</title><link href="/2023/12/13/PAM.html" rel="alternate" type="text/html" title="Data Augmentation for Traffic Classification accepted at PAM 2024" /><published>2023-12-13T00:00:00+00:00</published><updated>2023-12-13T00:00:00+00:00</updated><id>/2023/12/13/PAM</id><content type="html" xml:base="/2023/12/13/PAM.html"><![CDATA[<p>Our abstract titled <em>“Data Augmentation for Traffic Classification”</em> will be presented at PAM 2024</p>

<p>Congrats to Wang Chao and other Co-authors.</p>

<p><em>Abstract:</em> Abstract. Data Augmentation (DA)—enriching training data by adding synthetic samples—is a technique widely adopted in the Computer Vision (CV) and Natural Language Processing (NLP) domains to improve models performance. Yet, DA has struggled to gain traction in networking contexts, particularly in Traffic Classification (TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation functions applied to 3 TC datasets across a variety of conditions. Our results (i) show that DA can reap benefits previously unexplored with (ii) augmentations acting on sequence order and masking being a better suit for TC and (iii) provide hints about why augmentations have positive or negative effects based on simple latent space analysis.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our abstract titled “Data Augmentation for Traffic Classification” will be presented at PAM 2024 Congrats to Wang Chao and other Co-authors. Abstract: Abstract. Data Augmentation (DA)—enriching training data by adding synthetic samples—is a technique widely adopted in the Computer Vision (CV) and Natural Language Processing (NLP) domains to improve models performance. Yet, DA has struggled to gain traction in networking contexts, particularly in Traffic Classification (TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation functions applied to 3 TC datasets across a variety of conditions. Our results (i) show that DA can reap benefits previously unexplored with (ii) augmentations acting on sequence order and masking being a better suit for TC and (iii) provide hints about why augmentations have positive or negative effects based on simple latent space analysis.]]></summary></entry><entry><title type="html">SPADA accepted at Conext 2023</title><link href="/2023/10/18/Conext.html" rel="alternate" type="text/html" title="SPADA accepted at Conext 2023" /><published>2023-10-18T00:00:00+00:00</published><updated>2023-10-18T00:00:00+00:00</updated><id>/2023/10/18/Conext</id><content type="html" xml:base="/2023/10/18/Conext.html"><![CDATA[<p>Our paper titled <em>“SPADA: A Sparse Approximate Data Structure representation for data plane per-flow monitoring”</em> will be presented at Conext 2023</p>

<p>Congrats to the team, especially Andrea, Raphael, and Gabriele.</p>

<p><em>Abstract:</em> Accurate per-flow monitoring is critical for precise network diagnosis, performance analysis, and network operation and management in general. However, the limited amount of memory available on modern programmable devices and the large number of active flows force practitioners to monitor only the most relevant flows with approximate data structures, limiting their view of network traffic. We argue that, due to the skewed nature of network traffic, such data structures are, in practice, heavily underutilized, i.e., sparse, thus wasting a significant amount of memory.</p>

<p>This paper proposes a Sparse Approximate Data Structure (SPADA) representation that leverages sparsity to reduce the memory footprint of per-flow monitoring systems in the data plane while preserving their original accuracy. SPADA representation can be integrated into a generic per-flow monitoring system and is suitable for several measurement use cases. We prototype SPADA in P4 for a commercial FPGA target and test our approach with a custom simulator that we make publicly available, on four real network traces over three different monitoring tasks. Our results show that SPADA achieves 2× to 11× memory footprint reduction with respect to the state-of-the-art while maintaining the same accuracy, or even improving it.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our paper titled “SPADA: A Sparse Approximate Data Structure representation for data plane per-flow monitoring” will be presented at Conext 2023 Congrats to the team, especially Andrea, Raphael, and Gabriele. Abstract: Accurate per-flow monitoring is critical for precise network diagnosis, performance analysis, and network operation and management in general. However, the limited amount of memory available on modern programmable devices and the large number of active flows force practitioners to monitor only the most relevant flows with approximate data structures, limiting their view of network traffic. We argue that, due to the skewed nature of network traffic, such data structures are, in practice, heavily underutilized, i.e., sparse, thus wasting a significant amount of memory. This paper proposes a Sparse Approximate Data Structure (SPADA) representation that leverages sparsity to reduce the memory footprint of per-flow monitoring systems in the data plane while preserving their original accuracy. SPADA representation can be integrated into a generic per-flow monitoring system and is suitable for several measurement use cases. We prototype SPADA in P4 for a commercial FPGA target and test our approach with a custom simulator that we make publicly available, on four real network traces over three different monitoring tasks. Our results show that SPADA achieves 2× to 11× memory footprint reduction with respect to the state-of-the-art while maintaining the same accuracy, or even improving it.]]></summary></entry><entry><title type="html">Prelimary work on generative data augmentation for traffic classification accepted at Conext SW 2023</title><link href="/2023/10/16/ConextSW.html" rel="alternate" type="text/html" title="Prelimary work on generative data augmentation for traffic classification accepted at Conext SW 2023" /><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><id>/2023/10/16/ConextSW</id><content type="html" xml:base="/2023/10/16/ConextSW.html"><![CDATA[<p>Our abstract titled <em>“Toward Generative Data Augmentation for Traffic Classification”</em> will be presented at Conext Student Workshop 2023</p>

<p>Congrats to Chao Wang.</p>

<p><em>Abstract:</em> Data Augmentation (DA)—augmenting training data with synthetic samples—is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (𝑖) show that DA can reap benefits previously unexplored in TC and (𝑖𝑖) foster a research agenda on the use of generative models to automate DA design.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our abstract titled “Toward Generative Data Augmentation for Traffic Classification” will be presented at Conext Student Workshop 2023 Congrats to Chao Wang. Abstract: Data Augmentation (DA)—augmenting training data with synthetic samples—is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (𝑖) show that DA can reap benefits previously unexplored in TC and (𝑖𝑖) foster a research agenda on the use of generative models to automate DA design.]]></summary></entry><entry><title type="html">Prelimary work on learned data structures presented at Conext SW 2022</title><link href="/2022/12/06/ConextSW.html" rel="alternate" type="text/html" title="Prelimary work on learned data structures presented at Conext SW 2022" /><published>2022-12-06T00:00:00+00:00</published><updated>2022-12-06T00:00:00+00:00</updated><id>/2022/12/06/ConextSW</id><content type="html" xml:base="/2022/12/06/ConextSW.html"><![CDATA[<p>Our abstract titled <em>“Learned data structures for per-flow measurements”</em> has been presented at Conext Student Workshop 2022</p>

<p><em>Abstract:</em> This work presents a generic framework that exploits learning to improve the quality of network measurements. The main idea is to reuse measures collected by the network monitoring tasks to train an ML model that learns some per-flow characteristics and improves the measurement quality re-configuring the memory according to the learned information. We applied this idea to two different monitoring tasks, we identify the main issues related to this approach and we present some preliminary results.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our abstract titled “Learned data structures for per-flow measurements” has been presented at Conext Student Workshop 2022 Abstract: This work presents a generic framework that exploits learning to improve the quality of network measurements. The main idea is to reuse measures collected by the network monitoring tasks to train an ML model that learns some per-flow characteristics and improves the measurement quality re-configuring the memory according to the learned information. We applied this idea to two different monitoring tasks, we identify the main issues related to this approach and we present some preliminary results.]]></summary></entry><entry><title type="html">Paper accepted at HotNets 2022</title><link href="/2022/09/21/HotNets.html" rel="alternate" type="text/html" title="Paper accepted at HotNets 2022" /><published>2022-09-21T00:00:00+00:00</published><updated>2022-09-21T00:00:00+00:00</updated><id>/2022/09/21/HotNets</id><content type="html" xml:base="/2022/09/21/HotNets.html"><![CDATA[<p>Our paper titled <em>“Towards a systematic multi-modal representation learning for network data”</em> has been accepted at HotNets 2022</p>

<p><em>Abstract:</em> Learning the right representations from complex input data is the key ability of successful machine learning (ML) models. The latter are often tailored to a specific data modality. For example, recurrent neural networks (RNNs) were designed having the processing of sequential data in mind, while convolutional neural networks (CNNs) were designed to exploit spatial correlation naturally present in images. Unlike computer vision (CV) and natural language processing (NLP), each of which targets a single well-defined modality, network ML problems often have a mixture of data modalities as input. Yet, instead of exploiting such abundance, prac- titioners tend to rely on sub-features thereof, reducing the problem on single modality for the sake of simplicity.</p>

<p>In this paper, we advocate for exploiting all the modalities naturally present in network data. As a first step, we observe that network data systematically exhibits a mixture of quantities (e.g., measurements), and entities (e.g., IP addresses, names, etc.). Whereas the former are generally well exploited, the latter are often underused or poorly represented (e.g., with one-hot encoding). We propose to systematically leverage state of the art embedding techniques to learn entity representations, whenever significant sequences of such entities are historically observed. Through two diverse use cases, we show that such entity encoding can benefit and naturally augment classic quantity-based features.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our paper titled “Towards a systematic multi-modal representation learning for network data” has been accepted at HotNets 2022 Abstract: Learning the right representations from complex input data is the key ability of successful machine learning (ML) models. The latter are often tailored to a specific data modality. For example, recurrent neural networks (RNNs) were designed having the processing of sequential data in mind, while convolutional neural networks (CNNs) were designed to exploit spatial correlation naturally present in images. Unlike computer vision (CV) and natural language processing (NLP), each of which targets a single well-defined modality, network ML problems often have a mixture of data modalities as input. Yet, instead of exploiting such abundance, prac- titioners tend to rely on sub-features thereof, reducing the problem on single modality for the sake of simplicity. In this paper, we advocate for exploiting all the modalities naturally present in network data. As a first step, we observe that network data systematically exhibits a mixture of quantities (e.g., measurements), and entities (e.g., IP addresses, names, etc.). Whereas the former are generally well exploited, the latter are often underused or poorly represented (e.g., with one-hot encoding). We propose to systematically leverage state of the art embedding techniques to learn entity representations, whenever significant sequences of such entities are historically observed. Through two diverse use cases, we show that such entity encoding can benefit and naturally augment classic quantity-based features.]]></summary></entry><entry><title type="html">Paper accepted at INFOCOM 2022</title><link href="/2021/12/03/INFOCOM2022.html" rel="alternate" type="text/html" title="Paper accepted at INFOCOM 2022" /><published>2021-12-03T00:00:00+00:00</published><updated>2021-12-03T00:00:00+00:00</updated><id>/2021/12/03/INFOCOM2022</id><content type="html" xml:base="/2021/12/03/INFOCOM2022.html"><![CDATA[<p>Our paper titled <em>“Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching”</em>, will be presented at <a href="https://infocom2022.ieee-infocom.org/">INFOCOM 2022</a>. A preliminary version of the paper is available  <a href="https://arxiv.org/abs/2112.06671">here</a>. Full version <a href="https://gallomassimo.github.io/docs/2022Infocom.pdf">here</a></p>

<p><em>Abstract:</em> While Deep Learning (DL) technologies are a promising tool to solve networking problems that map to classification tasks, their computational complexity is still too high with respect to real-time traffic measurements requirements. To reduce the DL inference cost, we propose a novel caching paradigm, that we named approximate-key caching, which returns approximate results for lookups of selected input based on cached DL inference results. While approximate cache hits alleviate DL inference workload and increase the system throughput, they however introduce an approximation error. As such, we couple approximate-key caching with an error-correction principled algorithm, that we named auto-refresh. We analytically model our caching system performance for classic LRU and ideal caches, we perform a trace-driven evaluation of the expected performance, and we compare the benefits of our proposed approach with the state-of-the-art similarity caching – testifying the practical interest of our proposal.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our paper titled “Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching”, will be presented at INFOCOM 2022. A preliminary version of the paper is available here. Full version here]]></summary></entry><entry><title type="html">Hiring at Huawei Paris, Fall 2021</title><link href="/2021/09/01/Hiring.html" rel="alternate" type="text/html" title="Hiring at Huawei Paris, Fall 2021" /><published>2021-09-01T00:00:00+00:00</published><updated>2021-09-01T00:00:00+00:00</updated><id>/2021/09/01/Hiring</id><content type="html" xml:base="/2021/09/01/Hiring.html"><![CDATA[<p>My group has an opening for a permanent researcher position:</p>

<p>The Network Measurements research team of the Mathematical and Algorithmic Sciences Lab, is looking for candidates for a permanent research position on performance analysis, advanced data structure and network programmability to be applied in the context of Network measurements. The opening, is in the Huawei Research Center, located in the Paris area. The position focus on developing novel algorithms and mechanisms and/or provide accurate models for understanding and improving network measurements efficiency.</p>

<p>Postition has been filled stay tuned for more.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[My group has an opening for a permanent researcher position:]]></summary></entry><entry><title type="html">Paper accepted at SEC 2021</title><link href="/2021/04/03/SEC2021.html" rel="alternate" type="text/html" title="Paper accepted at SEC 2021" /><published>2021-04-03T00:00:00+00:00</published><updated>2021-04-03T00:00:00+00:00</updated><id>/2021/04/03/SEC2021</id><content type="html" xml:base="/2021/04/03/SEC2021.html"><![CDATA[<p>Our paper titled “FENXI: Fast in-network analytics”, will be presented at <a href="http://acm-ieee-sec.org/2021/">SEC 2021 2022</a>. A preliminary version of the paper is available <a href="https://arxiv.org/abs/2105.11738">here</a>. Full version <a href="https://gallomassimo.github.io/docs/2021sec.pdf">here</a></p>

<p><em>Abstract:</em> Live traffic analysis at the first aggregation point in the ISP network enables the implementation of complex traffic engineering policies but is limited by the scarce processing capabilities, especially for Deep Learning (DL) based analytics. The introduction of specialized hardware accelerators i.e., Tensor Processing Unit (TPU), offers the opportunity to enhance the processing capabilities of network devices at the edge. Yet, to date, no packet processing pipeline is capable of offering DL-based analysis capabilities in the data-plane, without interfering with network operations.
In this paper, we present FENXI, a system to run complex analytics by leveraging TPU. The design of FENXI decouples forwarding operations and traffic analytics which operates at different granularities i.e., packet and flow levels. We conceive two independent modules that asynchronously communicate to exchange network data and analytics results, and design data structures to extract flow level statistics without impacting per-packet processing. We prototyped and evaluated FENXI on general-purpose servers considering both adversarial and realistic network conditions. Our analysis shows that FENXI can sustain 100 Gbps line rate traffic processing requiring only limited resources, while also dynamically adapting to variable network conditions.</p>]]></content><author><name> </name><email>first.last@huawei.com</email></author><summary type="html"><![CDATA[Our paper titled “FENXI: Fast in-network analytics”, will be presented at SEC 2021 2022. A preliminary version of the paper is available here. Full version here]]></summary></entry></feed>