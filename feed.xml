<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-04-04T14:50:30+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">MG</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>Massimo Gallo</name><email>first.last@huawei.com</email></author><entry><title type="html">Paper accepted at INFOCOM 2022</title><link href="http://localhost:4000/2021/12/03/INFOCOM2022.html" rel="alternate" type="text/html" title="Paper accepted at INFOCOM 2022" /><published>2021-12-03T00:00:00+01:00</published><updated>2021-12-03T00:00:00+01:00</updated><id>http://localhost:4000/2021/12/03/INFOCOM2022</id><content type="html" xml:base="http://localhost:4000/2021/12/03/INFOCOM2022.html">&lt;p&gt;Our paper titled “FENXI: Fast in-network analytics”, will be presented at &lt;a href=&quot;&quot;&gt;&lt;/a&gt;. A preliminary version of the paper is available here &lt;a href=&quot;https://arxiv.org/abs/2105.11738&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Abstract:&lt;/em&gt; Live traffic analysis at the first aggregation point in the ISP network enables the implementation of complex traffic engineering policies but is limited by the scarce processing capabilities, especially for Deep Learning (DL) based analytics. The introduction of specialized hardware accelerators i.e., Tensor Processing Unit (TPU), offers the opportunity to enhance the processing capabilities of network devices at the edge. Yet, to date, no packet processing pipeline is capable of offering DL-based analysis capabilities in the data-plane, without interfering with network operations.
In this paper, we present FENXI, a system to run complex analytics by leveraging TPU. The design of FENXI decouples forwarding operations and traffic analytics which operates at different granularities i.e., packet and flow levels. We conceive two independent modules that asynchronously communicate to exchange network data and analytics results, and design data structures to extract flow level statistics without impacting per-packet processing. We prototyped and evaluated FENXI on general-purpose servers considering both adversarial and realistic network conditions. Our analysis shows that FENXI can sustain 100 Gbps line rate traffic processing requiring only limited resources, while also dynamically adapting to variable network conditions.&lt;/p&gt;</content><author><name>Massimo Gallo</name><email>first.last@huawei.com</email></author><summary type="html">Our paper titled “FENXI: Fast in-network analytics”, will be presented at . A preliminary version of the paper is available here here.</summary></entry><entry><title type="html">Hiring at Huawei Paris, Fall 2021</title><link href="http://localhost:4000/2021/09/01/Hiring.html" rel="alternate" type="text/html" title="Hiring at Huawei Paris, Fall 2021" /><published>2021-09-01T00:00:00+02:00</published><updated>2021-09-01T00:00:00+02:00</updated><id>http://localhost:4000/2021/09/01/Hiring</id><content type="html" xml:base="http://localhost:4000/2021/09/01/Hiring.html">&lt;p&gt;My group has an opening for a permanent researcher position:&lt;/p&gt;

&lt;p&gt;The Network Measurements research team of the Mathematical and Algorithmic Sciences Lab, is looking for candidates for a permanent research position on performance analysis, advanced data structure and network programmability to be applied in the context of Network measurements. The opening, is in the Huawei Research Center, located in the Paris area. The position focus on developing novel algorithms and mechanisms and/or provide accurate models for understanding and improving network measurements efficiency.&lt;/p&gt;

&lt;p&gt;Contact me if interested.&lt;/p&gt;</content><author><name>Massimo Gallo</name><email>first.last@huawei.com</email></author><summary type="html">My group has an opening for a permanent researcher position:</summary></entry><entry><title type="html">Paper accepted at SEC 2021</title><link href="http://localhost:4000/2021/04/03/SEC2021.html" rel="alternate" type="text/html" title="Paper accepted at SEC 2021" /><published>2021-04-03T00:00:00+02:00</published><updated>2021-04-03T00:00:00+02:00</updated><id>http://localhost:4000/2021/04/03/SEC2021</id><content type="html" xml:base="http://localhost:4000/2021/04/03/SEC2021.html">&lt;p&gt;Our paper titled &lt;em&gt;“Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching”&lt;/em&gt;, will be presented at &lt;a href=&quot;&quot;&gt;&lt;/a&gt;. A preliminary version of the paper is available here &lt;a href=&quot;https://arxiv.org/abs/2112.06671&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Abstract:&lt;/em&gt; While Deep Learning (DL) technologies are a promising tool to solve networking problems that map to classification tasks, their computational complexity is still too high with respect to real-time traffic measurements requirements. To reduce the DL inference cost, we propose a novel caching paradigm, that we named approximate-key caching, which returns approximate results for lookups of selected input based on cached DL inference results. While approximate cache hits alleviate DL inference workload and increase the system throughput, they however introduce an approximation error. As such, we couple approximate-key caching with an error-correction principled algorithm, that we named auto-refresh. We analytically model our caching system performance for classic LRU and ideal caches, we perform a trace-driven evaluation of the expected performance, and we compare the benefits of our proposed approach with the state-of-the-art similarity caching – testifying the practical interest of our proposal.&lt;/p&gt;</content><author><name>Massimo Gallo</name><email>first.last@huawei.com</email></author><summary type="html">Our paper titled “Accelerating Deep Learning Classification with Error-controlled Approximate-key Caching”, will be presented at . A preliminary version of the paper is available here here.</summary></entry></feed>