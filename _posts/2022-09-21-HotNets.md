---
title:  "Paper accepted at HotNets 2022"
search: true
author_profile: true
classes: wide
---

Our paper titled *"Towards a systematic multi-modal representation learning for network data"* has been accepted at HotNets 2022

*Abstract:* Learning the right representations from complex input data is the key ability of successful machine learning (ML) models. The latter are often tailored to a specific data modality. For example, recurrent neural networks (RNNs) were designed having the processing of sequential data in mind, while convolutional neural networks (CNNs) were designed to exploit spatial correlation naturally present in images. Unlike computer vision (CV) and natural language processing (NLP), each of which targets a single well-defined modality, network ML problems often have a mixture of data modalities as input. Yet, instead of exploiting such abundance, prac- titioners tend to rely on sub-features thereof, reducing the problem on single modality for the sake of simplicity.

In this paper, we advocate for exploiting all the modalities naturally present in network data. As a first step, we observe that network data systematically exhibits a mixture of quantities (e.g., measurements), and entities (e.g., IP addresses, names, etc.). Whereas the former are generally well exploited, the latter are often underused or poorly represented (e.g., with one-hot encoding). We propose to systematically leverage state of the art embedding techniques to learn entity representations, whenever significant sequences of such entities are historically observed. Through two diverse use cases, we show that such entity encoding can benefit and naturally augment classic quantity-based features.